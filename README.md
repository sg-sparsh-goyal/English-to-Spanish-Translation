# English to Spanish Translation using Transformer

This project demonstrates the implementation of an English to Spanish translation model using a Transformer architecture. The project is divided into two main parts:

## 1. Building the Transformer Model

The Transformer model was constructed from scratch, leveraging the principles outlined in the original "Attention is All You Need" paper. The model includes:

- **Positional Encoding**: To inject sequence order information into the model.
- **Multi-Head Attention**: To focus on different parts of the input sentence.
- **Feedforward Networks**: To process the information extracted by the attention mechanism.
- **Layer Normalization and Dropout**: To ensure stable and regularized training.

## 2. Translation Process

Once the Transformer model was built, it was trained on a dataset for English to Spanish translation. The trained model can translate simple English sentences into Spanish, demonstrating the effectiveness of the Transformer architecture for sequence-to-sequence tasks.

## Project Overview

- **Objective**: Translate English sentences into Spanish using a Transformer model.
- **Model Components**: Encoder, Decoder, Multi-Head Attention, Positional Encoding.
- **Outcome**: A working translation model capable of basic sentence translation from English to Spanish.

## Demo Video

You can watch the demo video by [clicking here](https://youtu.be/95HO83FiLgE)
